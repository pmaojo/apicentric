use async_trait::async_trait;
use crate::errors::ApicentricResult;
use super::AiProvider;

/// Local AI provider backed by a small language model running on the user's machine.
///
/// In a real implementation this would load a model file (for example using
/// `llama-rs`) and run inference locally.  To keep the dependency footprint small
/// and avoid heavy downloads this adapter currently returns a placeholder YAML
/// document that echoes the provided prompt.
pub struct LocalAiProvider {
    /// Path to the model on disk. Currently unused but kept for future
    /// integration.
    pub model_path: String,
}

impl LocalAiProvider {
    pub fn new(model_path: String) -> Self {
        Self { model_path }
    }
}

#[async_trait]
impl AiProvider for LocalAiProvider {
    async fn generate_yaml(&self, prompt: &str) -> ApicentricResult<String> {
        // In a full implementation we would load the model and generate the YAML
        // response here.  For now we simply wrap the prompt into a basic YAML
        // structure so the rest of the pipeline can be exercised without an
        // actual model.
        Ok(format!("# Generated by local model\n# Prompt: {}\n", prompt))
    }
}
